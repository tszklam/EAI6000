{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAI6000 2023 Spring A Week 6 Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section One - Conceptual Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">[GRADED  TASK 1.1]</div>\n",
    "Please compare various language models - __RNN__, __LSTM__, __GRU__, __Transformer__, __BERT__, and __GPTs__ in your own understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RNNs__ are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or the spoken word. A significant advantage of RNNs is their ability to use their internal state (memory) to process sequences of inputs, allowing them to remember past information. \n",
    "\n",
    "__LSTMs__ are an extension of RNNs and were developed to combat the vanishing gradient problem. They have a more complex structure, with an internal state cell and three \"gate\" structures: the input gate, forget gate, and output gate. These help maintain or discard information in the cell state over long periods, allowing the model to learn longer dependencies.\n",
    "\n",
    "__GRUs__ are a simplification of LSTMs that combine the forget and input gates into a single \"update gate.\" They also merge the cell state and hidden state, resulting in a lighter model than LSTM. \n",
    "\n",
    "The __Transformer__ model, introduced in the \"Attention is All You Need\" paper, discards recurrence and instead uses self-attention mechanisms that directly model relationships between all words in a sentence, no matter how far apart. It's highly parallelizable (leading to faster training times) and capable of capturing long-range dependencies.\n",
    "\n",
    "__BERT__ is based on the Transformer model. Unlike many earlier models, it is bidirectional, meaning that it uses both preceding and following context in understanding a word in a sentence.\n",
    "\n",
    "__GPT__, like BERT, is based on the Transformer model, but it is unidirectional, meaning it uses only the preceding context to understand a word. Its main innovation is using unsupervised pre-training, where it learns to predict the next word in a sentence, followed by fine-tuning for specific tasks. It has significantly more parameters than BERT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">[GRADED  TASK 1.2]</div>\n",
    "\n",
    "Please discuss different ways (at least three methods) to encode natural languages in your own understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __One-Hot Encoding__:\n",
    "\n",
    "This is the simplest and most straightforward method. In one-hot encoding, each word in the vocabulary is represented by a vector in n-dimensional space, where n is the size of the vocabulary. This vector is filled with 0s, except for a single 1 at the index corresponding to the word's position in the vocabulary.\n",
    "\n",
    "2. __Bag of Words (BoW)__:\n",
    "\n",
    "The Bag of Words model represents text as an 'unordered bag' or 'multiset' of its words, disregarding grammar and even word order but keeping track of frequency. Each document is represented as a vector in an n-dimensional space, where n is the size of the vocabulary. The value in each position in the vector corresponds to the frequency of that word in the document.\n",
    "\n",
    "3. __Word Embeddings__ :\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. These are dense vector representations, as opposed to sparse representations like one-hot encoding or BoW. Two popular methods for generating word embeddings are Word2Vec and GloVe.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Two - Sentiment Analysis\n",
    "\n",
    "Use the attached IMDB Dataset.csv file to run sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">[GRADED  TASK 2.1]</div>\n",
    "* Split the data into training and testing part using the `train_test_split` function so that the training set size is 75% of the whole data (set argument `random_state=2023` to make the result deterministic, and make sure the data is split in a stratified fashion)\n",
    "\n",
    "* Report and interpret the result (accuracy score) on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89904\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data set\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "X = data['review']  # Text data\n",
    "y = data['sentiment']  # Target labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2023, stratify=y)\n",
    "\n",
    "# vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# make predictions and testing\n",
    "y_pred = classifier.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accuracy score of 0.89904, it appears that the sentiment analysis model achieved a relatively high accuracy on the test set. An accuracy of 0.89904 indicates that the model correctly predicted the sentiment of approximately 89.9% of the test samples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">[GRADED  TASK 2.2]</div>\n",
    "* Try to add cross-validation using the `RepeateKFold` function with 5 splits, 10 repeats, and 2023 as random state. \n",
    "* Report the result on both training and test set with average and the standard deviation of the accuracy score\n",
    "* Please expain whether the model is overfitting or underfitting the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - Mean Accuracy: 0.9893\n",
      "Training Set - Standard Deviation: 0.0004\n",
      "Test Set - Mean Accuracy: 0.8756\n",
      "Test Set - Standard Deviation: 0.0056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "X = data['review']  # Text data\n",
    "y = data['sentiment']  # Target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2023, stratify=y)\n",
    "\n",
    "#vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "classifier = LinearSVC()\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=2023)\n",
    "\n",
    "\n",
    "train_scores = []\n",
    "for train_index, val_index in cv.split(X_train_vec):\n",
    "    X_train_cv, X_val = X_train_vec[train_index], X_train_vec[val_index]\n",
    "    y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    classifier.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_train = classifier.predict(X_train_cv)\n",
    "    train_accuracy = accuracy_score(y_train_cv, y_pred_train)\n",
    "    train_scores.append(train_accuracy)\n",
    "\n",
    "\n",
    "test_scores = []\n",
    "for train_index, val_index in cv.split(X_test_vec):\n",
    "    X_train_cv, X_val = X_test_vec[train_index], X_test_vec[val_index]\n",
    "    y_train_cv, y_val = y_test.iloc[train_index], y_test.iloc[val_index]\n",
    "    classifier.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_test = classifier.predict(X_val)\n",
    "    test_accuracy = accuracy_score(y_val, y_pred_test)\n",
    "    test_scores.append(test_accuracy)\n",
    "\n",
    "\n",
    "print(\"Training Set - Mean Accuracy:\", round(sum(train_scores) / len(train_scores), 4))\n",
    "print(\"Training Set - Standard Deviation:\", round(pd.Series(train_scores).std(), 4))\n",
    "\n",
    "print(\"Test Set - Mean Accuracy:\", round(sum(test_scores) / len(test_scores), 4))\n",
    "print(\"Test Set - Standard Deviation:\", round(pd.Series(test_scores).std(), 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved a high mean accuracy of 0.9893 on the training set, with a very low standard deviation of 0.0004. This indicates that the model performs exceptionally well on the training data. The small standard deviation suggests that the model's performance is consistent across different cross-validation folds.\n",
    "\n",
    "On the other hand, the model achieved a slightly lower mean accuracy of 0.8756 on the test set, with a higher standard deviation of 0.0056. The slightly lower accuracy compared to the training set suggests a small degree of __overfitting__. The standard deviation of 0.0056 on the test set indicates some variability in the model's performance across different cross-validation folds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
